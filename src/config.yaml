---
# general
verbose: 1 # how much info is printed: 0=none, 1=little, 2=all

# chat model
model:
    ID: # model ID from HuggingFace
        #Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF       # file size = 9.76 GB
        Qwen/Qwen2.5-Coder-1.5B-Instruct            # file size = 3.1 GB
        #Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ        # file size = 1.63 GB
        #Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4  # file size = 1.16 GB
    raw_model: true
    quant_type: full # True if the model is loaded directly, False if loaded through pipeline
    # valid values: ("full", "AWQ", "GPTQ", "bits") --> check file formats!!!

    # NOTE: the model can be loaded as quantized only if someone published the quantized version (see file's extensions):
    #       --> full precision: model.safetensors                 (direct load)
    #       --> full precision: pytorch_model.bin.index.json      (direct load, NOT with Transformers!)
    #       --> 4bit quantized: model-4bit.safetensors            (needs bitsandbytes)
    #       --> GPTQ quantized: gptq_model-4bit-128g.safetensors  (direct load)
    #       --> GGUF quantized: model.gguf                        (direct load, NOT with Transformers!)

# embedding model for building the database (for RAG)
emb_model: sentence-transformers/msmarco-bert-base-dot-v5  # model ID from HuggingFace

# database (RAG)
docs_dir: test-code # where I save the files for RAG
db_dir:   test-db   # where I save the vector database (db)
update_db: false # if I want to update the db (for example because I changed some files)
